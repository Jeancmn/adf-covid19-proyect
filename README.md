# Pipeline de Datos COVID-19 Europa - Proyecto Azure Data Factory

## ğŸ“‹ DescripciÃ³n General del Proyecto
Este proyecto implementa un pipeline **ETL End-to-End (Extraer, Transformar y Cargar)** completo utilizando **Azure Data Factory** para procesar datos de COVID-19 de Europa. Los datos provienen de **EuroStat** y del **Centro Europeo para la PrevenciÃ³n y Control de Enfermedades (ECDC)** y sigue una arquitectura de progresiÃ³n de calidad de datos **Bronce â†’ Plata â†’ Oro**.

El objetivo principal es ingerir datos crudos de COVID-19, realizar transformaciones y cargar los datos procesados en Azure SQL Database para anÃ¡lisis y visualizaciÃ³n usando herramientas como Power BI o Tableau.

---

## ğŸ—‚ï¸ Fuentes de Datos

| Nombre del Archivo | Fuente | DescripciÃ³n |
|-----------|--------|-------------|
| **population_by_age.tsv.gz** | EuroStat/Azure Blob Storage | Archivo comprimido que contiene datos de poblaciÃ³n por grupos de edad para paÃ­ses europeos |
| **cases_deaths.csv** | ECDC | Casos y muertes diarias emergentes de COVID-19 por paÃ­s |
| **hospital_admissions.csv** | ECDC | Admisiones hospitalarias diarias y semanales, admisiones a UCI por 100k de poblaciÃ³n |
| **testing.csv** | ECDC | Datos de pruebas semanales incluyendo cantidad de pruebas, tasa de testeo y tasa de positividad |
| **lookups** | Azure Blob Storage | Archivos de dimensiones incluyendo tablas de calendario y paÃ­ses |

---

## ğŸ—ï¸ Arquitectura General

El proyecto implementa una **arquitectura medallÃ³n** con tres capas de calidad de datos:

- **ğŸŸ¤ Capa Bronce**: Datos crudos ingeridos desde los sistemas fuente (Azure Blob Storage, endpoints HTTP)
- **ğŸ¥ˆ Capa Plata**: Datos limpios y transformados almacenados en ADLS Gen2
- **ğŸ¥‡ Capa Oro**: Datos listos para analÃ­tica cargados en Azure SQL Database

### Servicios de Azure Utilizados:
- **Azure Data Factory**: OrquestaciÃ³n y pipelines ETL
- **Azure Blob Storage**: Almacenamiento de datos crudos de poblaciÃ³n
- **Azure Data Lake Storage Gen2**: Almacenamiento de datos procesados
- **Azure SQL Database**: Base de datos final de analÃ­tica
- **Azure Databricks**: (Opcional) Para transformaciones avanzadas

---

## ğŸ“Š Flujo de Trabajo del Proyecto

### **Paso 1: Ingestar Datos de PoblaciÃ³n desde Blob Storage**

El archivo `population_by_age.tsv.gz` se carga manualmente a Azure Blob Storage. 

**Pipeline**: `pl_ingest_population_data`

**Actividades**:
1. **Actividad de ValidaciÃ³n**: Verifica si el archivo existe en Blob Storage
2. **Actividad Get Metadata**: Obtiene propiedades del archivo (tamaÃ±o, cantidad de columnas)
3. **CondiciÃ³n If**: Valida que el archivo cumple con los requisitos
4. **Actividad Copy**: Copia datos desde Blob Storage al contenedor raw de ADLS Gen2

<img width="2287" height="1202" alt="image" src="https://github.com/user-attachments/assets/6f65ee66-f5a3-4fba-95d2-7aa9221dbb09" />


**Linked Services**:
- `Ls_ablob_covidreportingsa`: ConexiÃ³n a Azure Blob Storage
 <img width="2311" height="1182" alt="image" src="https://github.com/user-attachments/assets/fa6a95c8-27ee-4db7-836d-cb6f0561e7a5" />


- `ls_adls_covidreportingdl`: ConexiÃ³n a ADLS Gen2
<img width="2307" height="1028" alt="image" src="https://github.com/user-attachments/assets/455fa7d0-f1c3-4db4-b2a4-30ed616cf5e3" />



**Trigger**: `tr_population_data_arrived` (Blob Event Trigger - se activa cuando se crea el archivo)

---

### **Paso 2: Ingestar Datos ECDC vÃ­a HTTP**

Tres conjuntos de datos (cases_deaths, hospital_admissions, testing) se almacenan en un repositorio GitHub y se ingestan vÃ­a conexiÃ³n HTTP.

**Pipeline**: `pl_ingest_ecdc_data`

**Actividades**:
1. **Actividad Lookup**: Lee el archivo JSON `ds_ecdc_file_list` que contiene la lista de archivos a ingestar
2. **Actividad ForEach**: Itera sobre cada archivo en la lista
3. **Actividad Copy**: Descarga cada archivo CSV desde la fuente HTTP a ADLS Gen2
<img width="2306" height="1006" alt="image" src="https://github.com/user-attachments/assets/7e4a6e59-200f-40c0-a89b-e18290a4d2f4" />


**ParametrizaciÃ³n**: Utiliza datasets parametrizados para procesar dinÃ¡micamente mÃºltiples archivos con una Ãºnica definiciÃ³n de pipeline.

**Linked Service**: `ls_http_open_data_ecdc_europa_eu` (conexiÃ³n HTTP a la fuente de datos)

**Trigger**: `tr_ingest_ecdc_data` (Tumbling Window Trigger - se ejecuta cada 24 horas)

---

### **Paso 3: Transformar Datos usando Data Flows**

DespuÃ©s de la ingesta, los data flows realizan transformaciones complejas sobre los datos crudos.

#### **Data Flow: df_transform_cases_deaths**

**Transformaciones**:
- Filtrar datos para incluir solo paÃ­ses europeos
- Seleccionar campos requeridos
- Buscar informaciÃ³n de paÃ­ses desde la tabla de dimensiÃ³n
- Pivotear datos por indicador (casos/muertes)
- Ordenar y formatear para salida
<img width="2359" height="1043" alt="image" src="https://github.com/user-attachments/assets/41501f6a-5508-49da-9fea-18ce0ae0585a" />


**Fuente**: `ds_raw_cases_and_deaths`, `ds_country_lookup`  
**Destino**: `ds_processed_cases_and_deaths`

**Pipeline**: `pl_process_cases_and_deaths_data`
**Trigger**: `tr_process_cases_and_deaths_data`

---

#### **Data Flow: df_transform_hospital_admissions**

**Transformaciones**:
- Seleccionar campos requeridos de datos de admisiones hospitalarias
- Buscar cÃ³digos de paÃ­ses usando dimensiÃ³n de paÃ­ses
- Dividir datos en flujos Diarios y Semanales
- Unir con tabla de dimensiÃ³n de fechas
- Pivotear por tipo de indicador (admisiones hospitalarias/UCI)
- Ordenar datos cronolÃ³gicamente
- Generar salidas separadas para datos diarios y semanales
<img width="2524" height="933" alt="image" src="https://github.com/user-attachments/assets/80c7a1f1-2878-4c34-a285-db6523ec8f96" />


**Fuente**: `ds_raw_hospital_admissions`, `ds_country_lookup`, `ds_dim_date_lookup`  
**Destinos**: `ds_processed_hospital_admission_daily`, `ds_processed_hospital_admission_weekly`

**Pipeline**: `pl_process_hospital_admission_data`  
**Trigger**: `tr_process_hospital_admission_data`

---

### **Paso 4: Cargar Datos a SQL Database (Sqlize)**

El paso final carga los datos transformados desde ADLS Gen2 a Azure SQL Database para reportes y anÃ¡lisis.

#### **Pipeline: pl_sqlize_cases_and_deaths_data**

**Actividades**:
- **Actividad Copy**: Carga datos procesados de casos y muertes en la tabla `covid_reporting.cases_and_deaths`
- **Script Pre-copy**: Trunca la tabla antes de cargar para asegurar datos limpios
- **Comportamiento de Escritura**: Modo insert
<img width="2242" height="1083" alt="image" src="https://github.com/user-attachments/assets/b031ab4e-48ca-4c3b-913b-5f15f9ff28a8" />


**Fuente**: `ds_processed_cases_and_deaths` (ADLS Gen2)  
**Destino**: `ds_sql_cases_and_deaths` (Azure SQL Database)  
**Trigger**: `tr_sqlize_cases_and_deaths_data`

#### **Pipeline: pl_sqlize_hospital_admission_data**

**Actividades**:
- Carga datos de admisiones hospitalarias diarias a SQL
- Carga datos de admisiones hospitalarias semanales a SQL
<img width="2154" height="1127" alt="image" src="https://github.com/user-attachments/assets/48fd7d00-5723-4b67-8fee-a2b96d1734fd" />


**Linked Service**: `ls_sql_covid_db` (conexiÃ³n a Azure SQL Database)  
**Trigger**: `tr_sqlize_hospital_admissions_data`

#### **Pipeline: pl_sqlize_testing**

**Actividades**:
- Carga datos de pruebas a SQL Database

**Destino**: `ds_sql_testing`

---

## ğŸ”„ Triggers y OrquestaciÃ³n

### Triggers Implementados:

| Nombre del Trigger | Tipo | PropÃ³sito | ProgramaciÃ³n |
|--------------|------|---------|----------|
| `tr_ingest_ecdc_data` | Tumbling Window | Ingesta datos ECDC desde HTTP | Cada 24 horas |
| `tr_population_data_arrived` | Blob Event | Se dispara cuando llega el archivo de poblaciÃ³n | Basado en eventos |
| `tr_process_cases_and_deaths_data` | Event/Schedule | Procesa datos de casos y muertes | DespuÃ©s de ingesta |
| `tr_process_hospital_admission_data` | Event/Schedule | Procesa datos hospitalarios | DespuÃ©s de ingesta |
| `tr_sqlize_cases_and_deaths_data` | Event/Schedule | Carga datos a SQL | DespuÃ©s de transformaciÃ³n |
| `tr_sqlize_hospital_admissions_data` | Event/Schedule | Carga datos hospitalarios a SQL | DespuÃ©s de transformaciÃ³n |

---

## ğŸ“ Estructura del Proyecto

```
adf-covid19-proyect/
â”œâ”€â”€ dataflow/                              # LÃ³gica de transformaciÃ³n de datos
â”‚   â”œâ”€â”€ df_transform_cases_deaths.json
â”‚   â””â”€â”€ df_transform_hospital_admissions.json
â”œâ”€â”€ dataset/                               # Definiciones de datasets
â”‚   â”œâ”€â”€ ds_country_lookup.json
â”‚   â”œâ”€â”€ ds_dim_date_lookup.json
â”‚   â”œâ”€â”€ ds_ecdc_file_list.json
â”‚   â”œâ”€â”€ ds_ecdc_raw_csv_http.json
â”‚   â”œâ”€â”€ ds_population_raw_gz.json
â”‚   â”œâ”€â”€ ds_processed_cases_and_deaths.json
â”‚   â”œâ”€â”€ ds_processed_hospital_admission_daily.json
â”‚   â”œâ”€â”€ ds_processed_hospital_admission_weekly.json
â”‚   â”œâ”€â”€ ds_processed_testing.json
â”‚   â”œâ”€â”€ ds_sql_cases_and_deaths.json
â”‚   â”œâ”€â”€ ds_sql_hospital_admissions_daily.json
â”‚   â””â”€â”€ ds_sql_testing.json
â”œâ”€â”€ linkedService/                         # Definiciones de conexiones
â”‚   â”œâ”€â”€ Ls_ablob_covidreportingsa.json    # Blob Storage
â”‚   â”œâ”€â”€ ls_adls_covidreportingdl.json     # ADLS Gen2
â”‚   â”œâ”€â”€ ls_http_open_data_ecdc_europa_eu.json  # Fuente HTTP
â”‚   â””â”€â”€ ls_sql_covid_db.json              # Azure SQL Database
â”œâ”€â”€ pipeline/                              # OrquestaciÃ³n de pipelines
â”‚   â”œâ”€â”€ pl_ingest_population_data.json
â”‚   â”œâ”€â”€ pl_ingest_ecdc_data.json
â”‚   â”œâ”€â”€ pl_process_cases_and_deaths_data.json
â”‚   â”œâ”€â”€ pl_process_hospital_admission_data.json
â”‚   â”œâ”€â”€ pl_sqlize_cases_and_deaths_data.json
â”‚   â”œâ”€â”€ pl_sqlize_hospital_admission_data.json
â”‚   â””â”€â”€ pl_sqlize_testing.json
â”œâ”€â”€ trigger/                               # Definiciones de triggers
â”‚   â”œâ”€â”€ tr_ingest_ecdc_data.json
â”‚   â”œâ”€â”€ tr_population_data_arrived.json
â”‚   â”œâ”€â”€ tr_process_cases_and_deaths_data.json
â”‚   â”œâ”€â”€ tr_process_hospital_admission_data.json
â”‚   â”œâ”€â”€ tr_sqlize_cases_and_deaths_data.json
â”‚   â””â”€â”€ tr_sqlize_hospital_admissions_data.json
â”œâ”€â”€ main-csv-data-files/                   # Archivos de datos de muestra
â”‚   â”œâ”€â”€ cases_deaths.csv
â”‚   â”œâ”€â”€ hospital_admissions.csv
â”‚   â””â”€â”€ testing.csv
â””â”€â”€ README.md
```

---

## ğŸš€ CaracterÃ­sticas Clave

âœ… **Ingesta Automatizada de Datos**: Triggers basados en horarios y eventos  
âœ… **Pipelines Parametrizados**: LÃ³gica de pipeline reutilizable con parÃ¡metros dinÃ¡micos  
âœ… **ValidaciÃ³n de Calidad de Datos**: Verificaciones de existencia de archivos y metadatos  
âœ… **Transformaciones Complejas**: Filtrado, pivoteo, uniones y agregaciones  
âœ… **Carga Incremental**: PatrÃ³n de truncar y cargar para tablas SQL  
âœ… **Manejo de Errores**: PolÃ­ticas de reintentos y gestiÃ³n de dependencias  
âœ… **Arquitectura Escalable**: Sigue arquitectura medallÃ³n (Bronce â†’ Plata â†’ Oro)

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Azure Data Factory**: OrquestaciÃ³n ETL
- **Azure Blob Storage**: Almacenamiento de datos crudos
- **Azure Data Lake Storage Gen2**: Almacenamiento de datos procesados
- **Azure SQL Database**: Base de datos de analÃ­tica
- **Data Flows**: DiseÃ±ador visual de transformaciones
- **HTTP Linked Service**: Ingesta de datos externos
- **JSON**: ConfiguraciÃ³n y metadatos

---

## ğŸ“Š Casos de Uso

Una vez que los datos se cargan en Azure SQL Database, se pueden usar para:

- **AnÃ¡lisis de Datos**: Consultar tendencias de COVID-19 usando SQL
- **Dashboards**: Crear visualizaciones con Power BI o Tableau
- **Machine Learning**: Usar datos procesados para modelado predictivo
- **Reportes**: Generar reportes automatizados para stakeholders

---


**ğŸ“Š Dashboards con Power BI**
<img width="2557" height="1301" alt="image" src="https://github.com/user-attachments/assets/d811e94f-f91f-4962-857e-c27421b302cb" />

<img width="2559" height="1304" alt="image" src="https://github.com/user-attachments/assets/13ac2377-c428-4728-853c-1ea367de6dda" />

<img width="2559" height="1263" alt="image" src="https://github.com/user-attachments/assets/e0795109-0789-492b-a015-65aac2a98879" />


## ğŸ“ Notas

- El proyecto sigue **mejores prÃ¡cticas** de ingenierÃ­a de datos con clara separaciÃ³n de etapas de ingesta, transformaciÃ³n y carga
- La **parametrizaciÃ³n** se utiliza extensivamente para hacer los pipelines reutilizables y mantenibles
- La **arquitectura basada en eventos** asegura que los datos se procesen tan pronto como llegan
- La calidad de datos mejora a travÃ©s de cada capa: **Bronce (crudo) â†’ Plata (limpio) â†’ Oro (listo para analÃ­tica)**

---

## ğŸ‘¤ Autor

**Jean Mangones**

---

## ğŸ“š Referencias

- [DocumentaciÃ³n de Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/)
- [Datos COVID-19 ECDC](https://www.ecdc.europa.eu/en/covid-19/data)
- [Datos de PoblaciÃ³n EuroStat](https://ec.europa.eu/eurostat)

---

## ğŸ“„ Licencia

Este proyecto es para propÃ³sitos educativos y de portafolio.

